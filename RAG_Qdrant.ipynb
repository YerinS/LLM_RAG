{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a59e53-510c-4960-aee7-4dd22c2ff0b4",
   "metadata": {},
   "source": [
    "## 1. API키 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f802dc-2030-4312-a790-5d2efaef7bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"openai.env\")\n",
    "\n",
    "#import os\n",
    "#print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a66c7-5986-41ac-bbd0-df405551bd64",
   "metadata": {},
   "source": [
    "## 2. 목차 추출\n",
    "#### 메타데이터로 활용하기 위함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b77e6b-9f6d-4cdf-a110-149b25a9f9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 Main Headings (8):\n",
      "['Table of contents', 'Introduction', 'Prompt engineering', 'LLM output configuration', 'Prompting techniques', 'Best Practices', 'Summary', 'Endnotes']\n",
      "\n",
      "📗 Sub Headings (25):\n",
      "['Output length', 'Sampling controls', 'General prompting / zero shot', 'One-shot & few-shot', 'System, contextual and role prompting', 'Step-back prompting', 'Chain of Thought (CoT)', 'Self-consistency', 'Tree of Thoughts (ToT)', 'ReAct (reason & act)', 'Automatic Prompt Engineering', 'Code prompting', 'Provide examples', 'Design with simplicity', 'Be specific about the output', 'Use Instructions over Constraints', 'Control the max token length', 'Use variables in prompts', 'Experiment with input formats and writing styles', 'For few-shot prompting with classification tasks, mix up the classes', 'Adapt to model updates', 'Experiment with output formats', 'Experiment together with other prompt engineers', 'CoT Best practices', 'Document the various prompt attempts']\n",
      "\n",
      "📙 Sub-Sub Headings (11):\n",
      "['Temperature', 'Top-K and top-P', 'Putting it all together', 'System prompting', 'Role prompting', 'Contextual prompting', 'Prompts for writing code', 'Prompts for explaining code', 'Prompts for translating code', 'Prompts for debugging and reviewing code', 'What about multimodal prompting?']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "pdf_path = \"./PromptEngineering.pdf\"\n",
    "toc_pages = [2, 3, 4]  # 페이지 3, 4, 5\n",
    "\n",
    "main_headings = []\n",
    "sub_headings = []\n",
    "sub_sub_headings = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page_num in toc_pages:\n",
    "        page = pdf.pages[page_num]\n",
    "        words = page.extract_words(use_text_flow=True)\n",
    "\n",
    "        lines = {}\n",
    "        for word in words:\n",
    "            top = round(word['top'])\n",
    "            x0 = word['x0']\n",
    "            text = word['text']\n",
    "            if top not in lines:\n",
    "                lines[top] = []\n",
    "            lines[top].append((x0, text))\n",
    "\n",
    "        for top in sorted(lines):\n",
    "            line = lines[top]\n",
    "            line.sort()  # 왼쪽에서 오른쪽 정렬\n",
    "            x0_first = line[0][0]\n",
    "            text_line = \" \".join(word for _, word in line)\n",
    "            text_line = re.sub(r'\\s+\\d+$', '', text_line).strip()\n",
    "\n",
    "            # 위치(x0 값) 기반으로 heading level 결정\n",
    "            if x0_first <= 75:\n",
    "                main_headings.append(text_line)\n",
    "            elif x0_first <= 100:\n",
    "                sub_headings.append(text_line)\n",
    "            else:\n",
    "                sub_sub_headings.append(text_line)\n",
    "\n",
    "# 출력 확인\n",
    "print(f\"📘 Main Headings ({len(main_headings)}):\\n{main_headings}\\n\")\n",
    "print(f\"📗 Sub Headings ({len(sub_headings)}):\\n{sub_headings}\\n\")\n",
    "print(f\"📙 Sub-Sub Headings ({len(sub_sub_headings)}):\\n{sub_sub_headings}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643642b-0cb6-4f6c-8f7e-9cb46392f22f",
   "metadata": {},
   "source": [
    "## 3. PDF 문서 읽어오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494f9f38-f2a8-4e80-bb0e-095bea18022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import pdfplumber\n",
    "\n",
    "documents = []\n",
    "\n",
    "# 페이지의 header/footer 제거용 crop 좌표 (상단 10%, 하단 10% 제거)\n",
    "crop_coords = [0, 0.1, 1, 0.9]\n",
    "\n",
    "# (텍스트 줄, 해당 페이지 번호)를 담는 리스트\n",
    "lines_with_page = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page_number, page in enumerate(pdf.pages, start=1):\n",
    "        cropped_width = page.width\n",
    "        cropped_height = page.height\n",
    "\n",
    "        cropped_bbox = (\n",
    "            crop_coords[0] * cropped_width,\n",
    "            crop_coords[1] * cropped_height,\n",
    "            crop_coords[2] * cropped_width,\n",
    "            crop_coords[3] * cropped_height,\n",
    "        )\n",
    "        page_crop = page.crop(bbox=cropped_bbox)\n",
    "\n",
    "        page_text = page_crop.extract_text()\n",
    "        if page_text:\n",
    "            lines = [line.strip() for line in page_text.split('\\n') if line.strip()]\n",
    "            for line in lines:\n",
    "                lines_with_page.append((line, page_number))\n",
    "\n",
    "# 문서 분리 기준용 변수 초기화\n",
    "current_main = None\n",
    "current_sub = None\n",
    "current_sub_sub = None\n",
    "current_content = \"\"\n",
    "current_page = None\n",
    "\n",
    "for line, page in lines_with_page:\n",
    "    if line in main_headings:\n",
    "        if current_content:\n",
    "            documents.append(Document(\n",
    "                page_content=current_content.strip(),\n",
    "                metadata={\n",
    "                    \"title\": pdf_path,\n",
    "                    \"page\": current_page,\n",
    "                    \"heading\": current_main,\n",
    "                    \"subheading\": current_sub or \"None\",\n",
    "                    \"sub_subheading\": current_sub_sub or \"None\"\n",
    "                }\n",
    "            ))\n",
    "            current_content = \"\"\n",
    "            current_sub = None\n",
    "            current_sub_sub = None\n",
    "        current_main = line\n",
    "        current_page = page\n",
    "\n",
    "    elif line in sub_headings:\n",
    "        if current_content:\n",
    "            documents.append(Document(\n",
    "                page_content=current_content.strip(),\n",
    "                metadata={\n",
    "                    \"title\": pdf_path,\n",
    "                    \"page\": current_page,\n",
    "                    \"heading\": current_main,\n",
    "                    \"subheading\": current_sub or \"None\",\n",
    "                    \"sub_subheading\": current_sub_sub or \"None\"\n",
    "                }\n",
    "            ))\n",
    "            current_content = \"\"\n",
    "            current_sub_sub = None\n",
    "        current_sub = line\n",
    "        current_page = page\n",
    "\n",
    "    elif line in sub_sub_headings:\n",
    "        if current_content:\n",
    "            documents.append(Document(\n",
    "                page_content=current_content.strip(),\n",
    "                metadata={\n",
    "                    \"title\": pdf_path,\n",
    "                    \"page\": current_page,\n",
    "                    \"heading\": current_main,\n",
    "                    \"subheading\": current_sub or \"None\",\n",
    "                    \"sub_subheading\": current_sub_sub or \"None\"\n",
    "                }\n",
    "            ))\n",
    "            current_content = \"\"\n",
    "        current_sub_sub = line\n",
    "        current_page = page\n",
    "\n",
    "    else:\n",
    "        current_content += line + \" \"\n",
    "        current_page = page  # 본문일 경우에도 현재 페이지로 업데이트\n",
    "\n",
    "# 마지막 청크 추가\n",
    "# 미실행시 마지막 청크가 누락되거나 빈 청크가 생길 수 있음\n",
    "if current_content:\n",
    "    documents.append(Document(\n",
    "        page_content=current_content.strip(),\n",
    "        metadata={\n",
    "            \"title\": pdf_path,\n",
    "            \"page\": current_page,\n",
    "            \"heading\": current_main,\n",
    "            \"subheading\": current_sub or \"None\",\n",
    "            \"sub_subheading\": current_sub_sub or \"None\"\n",
    "        }\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aff554-4e55-41e4-9e26-5f62d88ed817",
   "metadata": {},
   "source": [
    "## 4. Chunk 나누기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214e052-3d30-46dc-8444-b1b7b2504121",
   "metadata": {},
   "source": [
    "### 옵션1. 목차대로 나누기\n",
    "#### 장점 : 문서 작성자 의도대로 나눌 수 있음\n",
    "#### 단점 : 각 청크의 글자수 불균형 (최소 192자, 최대 6311자)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea916000-c2aa-4597-8039-471de7b09da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.text_splitter import CharacterTextSplitter\\n\\nsplitter = CharacterTextSplitter(separator=\"\\n\\n\")\\ndocs = splitter.split_documents(documents)\\n\\nprint(f\"총 chunk 수: {len(docs)}\")\\n# 각 청크의 글자 수 확인\\nfor i, doc in enumerate(docs, 1):\\n    print(f\"청크 {i}: {len(doc.page_content)}자\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(separator=\"\\n\\n\")\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"총 chunk 수: {len(docs)}\")\n",
    "# 각 청크의 글자 수 확인\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"청크 {i}: {len(doc.page_content)}자\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511c013-7f6d-4ba4-adb8-7ec81c9cba2c",
   "metadata": {},
   "source": [
    "### 옵션2. 500자 기준으로 나누기\n",
    "#### 장점 : 각 청크의 글자수 비교적 균형적, 옵션1에서 500자 이하인 청크는 그대로 유지됨\n",
    "#### 단점 : 연결된 내용이 서로 다른 청크들에 포함될 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1723955f-503a-4a8b-b3ef-c0f536f30f2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 chunk 수: 186\n",
      "청크 1: 291자\n",
      "청크 2: 497자\n",
      "청크 3: 498자\n",
      "청크 4: 389자\n",
      "청크 5: 488자\n",
      "청크 6: 497자\n",
      "청크 7: 410자\n",
      "청크 8: 499자\n",
      "청크 9: 497자\n",
      "청크 10: 487자\n",
      "청크 11: 370자\n",
      "청크 12: 252자\n",
      "청크 13: 494자\n",
      "청크 14: 296자\n",
      "청크 15: 439자\n",
      "청크 16: 491자\n",
      "청크 17: 494자\n",
      "청크 18: 351자\n",
      "청크 19: 493자\n",
      "청크 20: 498자\n",
      "청크 21: 285자\n",
      "청크 22: 494자\n",
      "청크 23: 494자\n",
      "청크 24: 497자\n",
      "청크 25: 492자\n",
      "청크 26: 498자\n",
      "청크 27: 416자\n",
      "청크 28: 498자\n",
      "청크 29: 88자\n",
      "청크 30: 494자\n",
      "청크 31: 496자\n",
      "청크 32: 490자\n",
      "청크 33: 496자\n",
      "청크 34: 153자\n",
      "청크 35: 498자\n",
      "청크 36: 496자\n",
      "청크 37: 499자\n",
      "청크 38: 487자\n",
      "청크 39: 494자\n",
      "청크 40: 418자\n",
      "청크 41: 499자\n",
      "청크 42: 498자\n",
      "청크 43: 497자\n",
      "청크 44: 305자\n",
      "청크 45: 496자\n",
      "청크 46: 495자\n",
      "청크 47: 497자\n",
      "청크 48: 493자\n",
      "청크 49: 499자\n",
      "청크 50: 205자\n",
      "청크 51: 499자\n",
      "청크 52: 496자\n",
      "청크 53: 497자\n",
      "청크 54: 493자\n",
      "청크 55: 496자\n",
      "청크 56: 496자\n",
      "청크 57: 498자\n",
      "청크 58: 68자\n",
      "청크 59: 496자\n",
      "청크 60: 496자\n",
      "청크 61: 499자\n",
      "청크 62: 80자\n",
      "청크 63: 499자\n",
      "청크 64: 499자\n",
      "청크 65: 499자\n",
      "청크 66: 498자\n",
      "청크 67: 492자\n",
      "청크 68: 492자\n",
      "청크 69: 491자\n",
      "청크 70: 493자\n",
      "청크 71: 492자\n",
      "청크 72: 495자\n",
      "청크 73: 498자\n",
      "청크 74: 497자\n",
      "청크 75: 321자\n",
      "청크 76: 496자\n",
      "청크 77: 496자\n",
      "청크 78: 499자\n",
      "청크 79: 498자\n",
      "청크 80: 499자\n",
      "청크 81: 494자\n",
      "청크 82: 496자\n",
      "청크 83: 498자\n",
      "청크 84: 494자\n",
      "청크 85: 494자\n",
      "청크 86: 273자\n",
      "청크 87: 493자\n",
      "청크 88: 494자\n",
      "청크 89: 497자\n",
      "청크 90: 496자\n",
      "청크 91: 497자\n",
      "청크 92: 494자\n",
      "청크 93: 496자\n",
      "청크 94: 497자\n",
      "청크 95: 499자\n",
      "청크 96: 496자\n",
      "청크 97: 494자\n",
      "청크 98: 497자\n",
      "청크 99: 496자\n",
      "청크 100: 470자\n",
      "청크 101: 490자\n",
      "청크 102: 496자\n",
      "청크 103: 97자\n",
      "청크 104: 498자\n",
      "청크 105: 492자\n",
      "청크 106: 498자\n",
      "청크 107: 488자\n",
      "청크 108: 495자\n",
      "청크 109: 495자\n",
      "청크 110: 494자\n",
      "청크 111: 485자\n",
      "청크 112: 496자\n",
      "청크 113: 499자\n",
      "청크 114: 496자\n",
      "청크 115: 491자\n",
      "청크 116: 217자\n",
      "청크 117: 192자\n",
      "청크 118: 499자\n",
      "청크 119: 494자\n",
      "청크 120: 495자\n",
      "청크 121: 495자\n",
      "청크 122: 497자\n",
      "청크 123: 140자\n",
      "청크 124: 499자\n",
      "청크 125: 497자\n",
      "청크 126: 493자\n",
      "청크 127: 499자\n",
      "청크 128: 281자\n",
      "청크 129: 497자\n",
      "청크 130: 498자\n",
      "청크 131: 480자\n",
      "청크 132: 495자\n",
      "청크 133: 278자\n",
      "청크 134: 492자\n",
      "청크 135: 499자\n",
      "청크 136: 462자\n",
      "청크 137: 498자\n",
      "청크 138: 497자\n",
      "청크 139: 499자\n",
      "청크 140: 497자\n",
      "청크 141: 491자\n",
      "청크 142: 498자\n",
      "청크 143: 474자\n",
      "청크 144: 498자\n",
      "청크 145: 497자\n",
      "청크 146: 173자\n",
      "청크 147: 383자\n",
      "청크 148: 251자\n",
      "청크 149: 465자\n",
      "청크 150: 499자\n",
      "청크 151: 446자\n",
      "청크 152: 495자\n",
      "청크 153: 62자\n",
      "청크 154: 489자\n",
      "청크 155: 497자\n",
      "청크 156: 495자\n",
      "청크 157: 496자\n",
      "청크 158: 68자\n",
      "청크 159: 230자\n",
      "청크 160: 499자\n",
      "청크 161: 408자\n",
      "청크 162: 496자\n",
      "청크 163: 495자\n",
      "청크 164: 498자\n",
      "청크 165: 153자\n",
      "청크 166: 294자\n",
      "청크 167: 493자\n",
      "청크 168: 282자\n",
      "청크 169: 296자\n",
      "청크 170: 498자\n",
      "청크 171: 232자\n",
      "청크 172: 500자\n",
      "청크 173: 488자\n",
      "청크 174: 494자\n",
      "청크 175: 494자\n",
      "청크 176: 499자\n",
      "청크 177: 491자\n",
      "청크 178: 111자\n",
      "청크 179: 497자\n",
      "청크 180: 86자\n",
      "청크 181: 489자\n",
      "청크 182: 470자\n",
      "청크 183: 443자\n",
      "청크 184: 495자\n",
      "청크 185: 492자\n",
      "청크 186: 101자\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"총 chunk 수: {len(docs)}\")\n",
    "# 각 청크의 글자 수 확인\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"청크 {i}: {len(doc.page_content)}자\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c30ec8-8dad-4a7b-a862-0a241fb4b8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': './PromptEngineering.pdf', 'page': 10, 'heading': 'LLM output configuration', 'subheading': 'Sampling controls', 'sub_subheading': 'Temperature'}\n"
     ]
    }
   ],
   "source": [
    "# heading, subheading, sub_subheading 확인\n",
    "print(docs[17].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728223ac-2c33-47c6-a618-fe0086607a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='temperature with high certainty. A higher Gemini temperature setting is like a high softmax temperature, making a wider range of temperatures around the selected setting more acceptable. This increased uncertainty accommodates scenarios where a rigid, precise temperature may not be essential like for example when experimenting with creative outputs.' metadata={'title': './PromptEngineering.pdf', 'page': 10, 'heading': 'LLM output configuration', 'subheading': 'Sampling controls', 'sub_subheading': 'Temperature'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2772a8-53a6-4095-b016-2cb75e96ad66",
   "metadata": {},
   "source": [
    "## 5. Qdrant 벡터DB 생성 또는 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec45151c-557f-4e48-b698-f4c4744e7197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 컬렉션 'qdrant_0526'에 연결합니다.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wiseai\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 0.3.0. An updated version of the class exists in the langchain-qdrant package and should be used instead. To use it run `pip install -U langchain-qdrant` and import as `from langchain_qdrant import Qdrant`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import CollectionStatus\n",
    "\n",
    "# 임베딩 모델 설정\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# 로컬 도커로 실행 중인 Qdrant에 연결\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# 현재 존재하는 컬렉션 목록 확인\n",
    "existing_collections = [col.name for col in client.get_collections().collections]\n",
    "#print(existing_collections)\n",
    "\n",
    "# 사용할 컬렉션 이름\n",
    "collection_name = \"qdrant_0526\"\n",
    "\n",
    "if collection_name in existing_collections:\n",
    "    print(f\"기존 컬렉션 '{collection_name}'에 연결합니다.\")\n",
    "    qdrant = Qdrant(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "else:\n",
    "    print(f\"컬렉션 '{collection_name}'을 생성하고 문서를 업로드합니다.\")\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        url=\"http://localhost:6333\",\n",
    "        prefer_grpc=True,  # gRPC 사용 가능하면 True 추천\n",
    "        collection_name=collection_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510728e7-60aa-44c4-9db4-3181c9230f41",
   "metadata": {},
   "source": [
    "## 6. 쿼리에 대한 답변 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425bc60-6e7a-445f-b7e2-54f8243890b5",
   "metadata": {},
   "source": [
    "#### Qdrant에서 유사 문서 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd39864f-6ee3-4a47-a2a7-21570a7b132b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- 문서 1 -------\n",
      "유사도 점수: 0.4728\n",
      "대목차: Prompting techniques\n",
      "중목차: Tree of Thoughts (ToT)\n",
      "소목차: None\n",
      "페이지: 37\n",
      "내용: Now that we are familiar with chain of thought and self-consistency prompting, let’s review Tree of Thoughts (ToT).12 It generalizes the concept of CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a single linear chain of thought. This is depicted in Figure 1. Figure 1. A visualization of chain of thought prompting on the left versus. Tree of Thoughts prompting on the right This approach makes ToT particularly\n",
      "\n",
      "------- 문서 2 -------\n",
      "유사도 점수: 0.4451\n",
      "대목차: Prompting techniques\n",
      "중목차: Chain of Thought (CoT)\n",
      "소목차: None\n",
      "페이지: 32\n",
      "내용: Chain of Thought (CoT) 9 prompting is a technique for improving the reasoning capabilities of LLMs by generating intermediate reasoning steps. This helps the LLM generate more accurate answers. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding as it’s a challenge with a zero-shot chain of thought. CoT has a lot of advantages. First of all, it’s low-effort while being very effective and works well with off-the-shelf\n",
      "\n",
      "------- 문서 3 -------\n",
      "유사도 점수: 0.4345\n",
      "대목차: Prompting techniques\n",
      "중목차: Tree of Thoughts (ToT)\n",
      "소목차: None\n",
      "페이지: 37\n",
      "내용: on the right This approach makes ToT particularly well-suited for complex tasks that require exploration. It works by maintaining a tree of thoughts, where each thought represents a coherent language sequence that serves as an intermediate step toward solving a problem. The model can then explore different reasoning paths by branching out from different nodes in the tree. There’s a great notebook, which goes into a bit more detail showing The Tree of Thought (ToT) which is based on the paper\n",
      "\n",
      "------- 문서 4 -------\n",
      "유사도 점수: 0.4219\n",
      "대목차: Best Practices\n",
      "중목차: CoT Best practices\n",
      "소목차: None\n",
      "페이지: 61\n",
      "내용: For CoT prompting, putting the answer after the reasoning is required because the generation of the reasoning changes the tokens that the model gets when it predicts the final answer. With CoT and self-consistency you need to be able to extract the final answer from your prompt, separated from the reasoning. For CoT prompting, set the temperature to 0. Chain of thought prompting is based on greedy decoding, predicting the next word in a sequence based on the highest probability assigned by the\n",
      "\n",
      "------- 문서 5 -------\n",
      "유사도 점수: 0.4108\n",
      "대목차: Prompting techniques\n",
      "중목차: Chain of Thought (CoT)\n",
      "소목차: None\n",
      "페이지: 32\n",
      "내용: chain of thought. Please refer to the notebook10 hosted in the GoogleCloudPlatform Github repository which will go into further detail on CoT prompting: In the best practices section of this chapter, we will learn some best practices specific to Chain of thought prompting.\n"
     ]
    }
   ],
   "source": [
    "query = \"CoT와 ToT를 비교해서 설명해줘\"\n",
    "retrieved_docs = qdrant.similarity_search_with_score(query, k=5)\n",
    "\n",
    "for i, (doc, score) in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n------- 문서 {i} -------\")\n",
    "    print(f\"유사도 점수: {score:.4f}\")\n",
    "    print(f\"대목차: {doc.metadata.get('heading')}\")\n",
    "    print(f\"중목차: {doc.metadata.get('subheading')}\")\n",
    "    print(f\"소목차: {doc.metadata.get('sub_subheading')}\")\n",
    "    print(f\"페이지: {doc.metadata.get('page')}\")\n",
    "    print(f\"내용: {doc.page_content[:1000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "263ab69d-2935-4027-991c-139699817a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5개 문서 검색에 성공했습니다.\n"
     ]
    }
   ],
   "source": [
    "# 유사도 점수 0.4 이상으로 필터링 및 유사도 점수 제거\n",
    "filtered_docs = [doc for doc, score in retrieved_docs if score >= 0.4]\n",
    "if not filtered_docs:\n",
    "    print(\"검색에 실패했습니다.\")\n",
    "else:\n",
    "    print(f\"{len(filtered_docs)}개 문서 검색에 성공했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03018c4-1826-477a-8dff-90194f7ba0cc",
   "metadata": {},
   "source": [
    "#### 답변 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f4bd382-2283-42ce-ab1b-4000575ae152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❓ 질문: CoT와 ToT를 비교해서 설명해줘\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "💡 답변: 네, Chain of Thought(CoT)와 Tree of Thoughts(ToT)를 비교해서 설명드리겠습니다.\n",
      "\n",
      "### 1. Chain of Thought (CoT)\n",
      "- **개념**: CoT는 대형 언어 모델(LLM)이 문제를 해결할 때, 중간 추론 과정을 단계별로 생성하도록 유도하는 프롬프트 기법입니다.\n",
      "- **방식**: 한 번에 한 경로(선형적, 일직선)의 추론만을 따라가며, 각 단계별로 논리적 이유를 차례로 나열합니다.\n",
      "- **장점**: 구현이 간단하고, 기존 LLM에 바로 적용할 수 있으며, 복잡한 문제에 대해 더 정확한 답변을 얻을 수 있습니다.\n",
      "- **적용 예시**: 수학 문제 풀이, 논리적 추론 문제 등에서 \"생각의 흐름\"을 단계별로 보여줍니다.\n",
      "\n",
      "### 2. Tree of Thoughts (ToT)\n",
      "- **개념**: ToT는 CoT를 일반화한 기법으로, LLM이 여러 개의 추론 경로(가지)를 동시에 탐색할 수 있도록 합니다.\n",
      "- **방식**: 문제를 해결하는 과정에서 하나의 선형 경로가 아니라, 여러 가지 가능한 중간 단계(노드)를 트리 구조로 확장하며 탐색합니다. 각 노드는 하나의 \"생각\"을 나타내고, 여러 노드에서 새로운 가지로 분기할 수 있습니다.\n",
      "- **장점**: 복잡하고 탐색이 필요한 문제(예: 창의적 문제 해결, 계획 수립 등)에 더 적합합니다. 다양한 경로를 동시에 고려하므로 더 나은 해답을 찾을 확률이 높아집니다.\n",
      "- **적용 예시**: 여러 가지 해결책을 비교하거나, 다양한 시나리오를 동시에 고려해야 하는 문제에 효과적입니다.\n",
      "\n",
      "---\n",
      "\n",
      "### 요약 비교\n",
      "\n",
      "| 구분         | Chain of Thought (CoT)         | Tree of Thoughts (ToT)           |\n",
      "|--------------|-------------------------------|-----------------------------------|\n",
      "| 추론 경로    | 한 개(선형)                   | 여러 개(트리 구조)                |\n",
      "| 탐색 방식    | 단계별로 한 방향만 진행        | 여러 방향으로 분기하며 탐색        |\n",
      "| 장점         | 간단, 효과적, 적용 쉬움        | 복잡한 문제에 강함, 다양한 해법 탐색 |\n",
      "| 활용 예시    | 수학 풀이, 논리 문제           | 창의적 문제, 계획, 시나리오 분석   |\n",
      "\n",
      "---\n",
      "\n",
      "**정리:**  \n",
      "CoT는 한 가지 생각의 흐름을 따라가며 문제를 푸는 방식이고, ToT는 여러 가지 생각의 흐름을 동시에 탐색하여 더 복잡한 문제에 적합한 방식입니다.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# LLM 초기화\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4.1\")\n",
    "\n",
    "# QA 체인 불러오기 (chain_type: \"stuff\", \"map_reduce\", \"refine\" 중 선택)\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "# 질문에 대한 답변 생성\n",
    "answer = qa_chain.invoke({\"input_documents\": filtered_docs, \"question\": query})\n",
    "\n",
    "print(\"❓ 질문:\", answer.get(\"question\", \"\"))\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "print(\"💡 답변:\", answer.get(\"output_text\", \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
