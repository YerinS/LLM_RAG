{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2a59e53-510c-4960-aee7-4dd22c2ff0b4",
   "metadata": {},
   "source": [
    "## 1. APIí‚¤ ê°€ì ¸ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89f802dc-2030-4312-a790-5d2efaef7bdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"openai.env\")\n",
    "\n",
    "#import os\n",
    "#print(os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a22a66c7-5986-41ac-bbd0-df405551bd64",
   "metadata": {},
   "source": [
    "## 2. ëª©ì°¨ ì¶”ì¶œ\n",
    "#### ë©”íƒ€ë°ì´í„°ë¡œ í™œìš©í•˜ê¸° ìœ„í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07b77e6b-9f6d-4cdf-a110-149b25a9f9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“˜ Main Headings (8):\n",
      "['Table of contents', 'Introduction', 'Prompt engineering', 'LLM output configuration', 'Prompting techniques', 'Best Practices', 'Summary', 'Endnotes']\n",
      "\n",
      "ğŸ“— Sub Headings (25):\n",
      "['Output length', 'Sampling controls', 'General prompting / zero shot', 'One-shot & few-shot', 'System, contextual and role prompting', 'Step-back prompting', 'Chain of Thought (CoT)', 'Self-consistency', 'Tree of Thoughts (ToT)', 'ReAct (reason & act)', 'Automatic Prompt Engineering', 'Code prompting', 'Provide examples', 'Design with simplicity', 'Be specific about the output', 'Use Instructions over Constraints', 'Control the max token length', 'Use variables in prompts', 'Experiment with input formats and writing styles', 'For few-shot prompting with classification tasks, mix up the classes', 'Adapt to model updates', 'Experiment with output formats', 'Experiment together with other prompt engineers', 'CoT Best practices', 'Document the various prompt attempts']\n",
      "\n",
      "ğŸ“™ Sub-Sub Headings (11):\n",
      "['Temperature', 'Top-K and top-P', 'Putting it all together', 'System prompting', 'Role prompting', 'Contextual prompting', 'Prompts for writing code', 'Prompts for explaining code', 'Prompts for translating code', 'Prompts for debugging and reviewing code', 'What about multimodal prompting?']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "\n",
    "pdf_path = \"./PromptEngineering.pdf\"\n",
    "toc_pages = [2, 3, 4]  # í˜ì´ì§€ 3, 4, 5\n",
    "\n",
    "main_headings = []\n",
    "sub_headings = []\n",
    "sub_sub_headings = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page_num in toc_pages:\n",
    "        page = pdf.pages[page_num]\n",
    "        words = page.extract_words(use_text_flow=True)\n",
    "\n",
    "        lines = {}\n",
    "        for word in words:\n",
    "            top = round(word['top'])\n",
    "            x0 = word['x0']\n",
    "            text = word['text']\n",
    "            if top not in lines:\n",
    "                lines[top] = []\n",
    "            lines[top].append((x0, text))\n",
    "\n",
    "        for top in sorted(lines):\n",
    "            line = lines[top]\n",
    "            line.sort()  # ì™¼ìª½ì—ì„œ ì˜¤ë¥¸ìª½ ì •ë ¬\n",
    "            x0_first = line[0][0]\n",
    "            text_line = \" \".join(word for _, word in line)\n",
    "            text_line = re.sub(r'\\s+\\d+$', '', text_line).strip()\n",
    "\n",
    "            # ìœ„ì¹˜(x0 ê°’) ê¸°ë°˜ìœ¼ë¡œ heading level ê²°ì •\n",
    "            if x0_first <= 75:\n",
    "                main_headings.append(text_line)\n",
    "            elif x0_first <= 100:\n",
    "                sub_headings.append(text_line)\n",
    "            else:\n",
    "                sub_sub_headings.append(text_line)\n",
    "\n",
    "# ì¶œë ¥ í™•ì¸\n",
    "print(f\"ğŸ“˜ Main Headings ({len(main_headings)}):\\n{main_headings}\\n\")\n",
    "print(f\"ğŸ“— Sub Headings ({len(sub_headings)}):\\n{sub_headings}\\n\")\n",
    "print(f\"ğŸ“™ Sub-Sub Headings ({len(sub_sub_headings)}):\\n{sub_sub_headings}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643642b-0cb6-4f6c-8f7e-9cb46392f22f",
   "metadata": {},
   "source": [
    "## 3. PDF ë¬¸ì„œ ì½ì–´ì˜¤ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "494f9f38-f2a8-4e80-bb0e-095bea18022c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema import Document\n",
    "import pdfplumber\n",
    "\n",
    "documents = []\n",
    "\n",
    "# í˜ì´ì§€ì˜ header/footer ì œê±°ìš© crop ì¢Œí‘œ (ìƒë‹¨ 10%, í•˜ë‹¨ 10% ì œê±°)\n",
    "crop_coords = [0, 0.1, 1, 0.9]\n",
    "\n",
    "# (í…ìŠ¤íŠ¸ ì¤„, í•´ë‹¹ í˜ì´ì§€ ë²ˆí˜¸)ë¥¼ ë‹´ëŠ” ë¦¬ìŠ¤íŠ¸\n",
    "lines_with_page = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for page_number, page in enumerate(pdf.pages, start=1):\n",
    "        cropped_width = page.width\n",
    "        cropped_height = page.height\n",
    "\n",
    "        cropped_bbox = (\n",
    "            crop_coords[0] * cropped_width,\n",
    "            crop_coords[1] * cropped_height,\n",
    "            crop_coords[2] * cropped_width,\n",
    "            crop_coords[3] * cropped_height,\n",
    "        )\n",
    "        page_crop = page.crop(bbox=cropped_bbox)\n",
    "\n",
    "        page_text = page_crop.extract_text()\n",
    "        if page_text:\n",
    "            lines = [line.strip() for line in page_text.split('\\n') if line.strip()]\n",
    "            for line in lines:\n",
    "                lines_with_page.append((line, page_number))\n",
    "\n",
    "# ë¬¸ì„œ ë¶„ë¦¬ ê¸°ì¤€ìš© ë³€ìˆ˜ ì´ˆê¸°í™”\n",
    "current_main = None\n",
    "current_sub = None\n",
    "current_sub_sub = None\n",
    "current_content = \"\"\n",
    "current_page = None\n",
    "\n",
    "for line, page in lines_with_page:\n",
    "    if line in main_headings:\n",
    "        if current_content:\n",
    "            documents.append(Document(\n",
    "                page_content=current_content.strip(),\n",
    "                metadata={\n",
    "                    \"title\": pdf_path,\n",
    "                    \"page\": current_page,\n",
    "                    \"heading\": current_main,\n",
    "                    \"subheading\": current_sub or \"None\",\n",
    "                    \"sub_subheading\": current_sub_sub or \"None\"\n",
    "                }\n",
    "            ))\n",
    "            current_content = \"\"\n",
    "            current_sub = None\n",
    "            current_sub_sub = None\n",
    "        current_main = line\n",
    "        current_page = page\n",
    "\n",
    "    elif line in sub_headings:\n",
    "        if current_content:\n",
    "            documents.append(Document(\n",
    "                page_content=current_content.strip(),\n",
    "                metadata={\n",
    "                    \"title\": pdf_path,\n",
    "                    \"page\": current_page,\n",
    "                    \"heading\": current_main,\n",
    "                    \"subheading\": current_sub or \"None\",\n",
    "                    \"sub_subheading\": current_sub_sub or \"None\"\n",
    "                }\n",
    "            ))\n",
    "            current_content = \"\"\n",
    "            current_sub_sub = None\n",
    "        current_sub = line\n",
    "        current_page = page\n",
    "\n",
    "    elif line in sub_sub_headings:\n",
    "        if current_content:\n",
    "            documents.append(Document(\n",
    "                page_content=current_content.strip(),\n",
    "                metadata={\n",
    "                    \"title\": pdf_path,\n",
    "                    \"page\": current_page,\n",
    "                    \"heading\": current_main,\n",
    "                    \"subheading\": current_sub or \"None\",\n",
    "                    \"sub_subheading\": current_sub_sub or \"None\"\n",
    "                }\n",
    "            ))\n",
    "            current_content = \"\"\n",
    "        current_sub_sub = line\n",
    "        current_page = page\n",
    "\n",
    "    else:\n",
    "        current_content += line + \" \"\n",
    "        current_page = page  # ë³¸ë¬¸ì¼ ê²½ìš°ì—ë„ í˜„ì¬ í˜ì´ì§€ë¡œ ì—…ë°ì´íŠ¸\n",
    "\n",
    "# ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€\n",
    "# ë¯¸ì‹¤í–‰ì‹œ ë§ˆì§€ë§‰ ì²­í¬ê°€ ëˆ„ë½ë˜ê±°ë‚˜ ë¹ˆ ì²­í¬ê°€ ìƒê¸¸ ìˆ˜ ìˆìŒ\n",
    "if current_content:\n",
    "    documents.append(Document(\n",
    "        page_content=current_content.strip(),\n",
    "        metadata={\n",
    "            \"title\": pdf_path,\n",
    "            \"page\": current_page,\n",
    "            \"heading\": current_main,\n",
    "            \"subheading\": current_sub or \"None\",\n",
    "            \"sub_subheading\": current_sub_sub or \"None\"\n",
    "        }\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aff554-4e55-41e4-9e26-5f62d88ed817",
   "metadata": {},
   "source": [
    "## 4. Chunk ë‚˜ëˆ„ê¸°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6214e052-3d30-46dc-8444-b1b7b2504121",
   "metadata": {},
   "source": [
    "### ì˜µì…˜1. ëª©ì°¨ëŒ€ë¡œ ë‚˜ëˆ„ê¸°\n",
    "#### ì¥ì  : ë¬¸ì„œ ì‘ì„±ì ì˜ë„ëŒ€ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆìŒ\n",
    "#### ë‹¨ì  : ê° ì²­í¬ì˜ ê¸€ììˆ˜ ë¶ˆê· í˜• (ìµœì†Œ 192ì, ìµœëŒ€ 6311ì)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea916000-c2aa-4597-8039-471de7b09da0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom langchain.text_splitter import CharacterTextSplitter\\n\\nsplitter = CharacterTextSplitter(separator=\"\\n\\n\")\\ndocs = splitter.split_documents(documents)\\n\\nprint(f\"ì´ chunk ìˆ˜: {len(docs)}\")\\n# ê° ì²­í¬ì˜ ê¸€ì ìˆ˜ í™•ì¸\\nfor i, doc in enumerate(docs, 1):\\n    print(f\"ì²­í¬ {i}: {len(doc.page_content)}ì\")\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "\n",
    "splitter = CharacterTextSplitter(separator=\"\\n\\n\")\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"ì´ chunk ìˆ˜: {len(docs)}\")\n",
    "# ê° ì²­í¬ì˜ ê¸€ì ìˆ˜ í™•ì¸\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"ì²­í¬ {i}: {len(doc.page_content)}ì\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3511c013-7f6d-4ba4-adb8-7ec81c9cba2c",
   "metadata": {},
   "source": [
    "### ì˜µì…˜2. 500ì ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ê¸°\n",
    "#### ì¥ì  : ê° ì²­í¬ì˜ ê¸€ììˆ˜ ë¹„êµì  ê· í˜•ì , ì˜µì…˜1ì—ì„œ 500ì ì´í•˜ì¸ ì²­í¬ëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ë¨\n",
    "#### ë‹¨ì  : ì—°ê²°ëœ ë‚´ìš©ì´ ì„œë¡œ ë‹¤ë¥¸ ì²­í¬ë“¤ì— í¬í•¨ë  ìˆ˜ ìˆìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1723955f-503a-4a8b-b3ef-c0f536f30f2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ chunk ìˆ˜: 186\n",
      "ì²­í¬ 1: 291ì\n",
      "ì²­í¬ 2: 497ì\n",
      "ì²­í¬ 3: 498ì\n",
      "ì²­í¬ 4: 389ì\n",
      "ì²­í¬ 5: 488ì\n",
      "ì²­í¬ 6: 497ì\n",
      "ì²­í¬ 7: 410ì\n",
      "ì²­í¬ 8: 499ì\n",
      "ì²­í¬ 9: 497ì\n",
      "ì²­í¬ 10: 487ì\n",
      "ì²­í¬ 11: 370ì\n",
      "ì²­í¬ 12: 252ì\n",
      "ì²­í¬ 13: 494ì\n",
      "ì²­í¬ 14: 296ì\n",
      "ì²­í¬ 15: 439ì\n",
      "ì²­í¬ 16: 491ì\n",
      "ì²­í¬ 17: 494ì\n",
      "ì²­í¬ 18: 351ì\n",
      "ì²­í¬ 19: 493ì\n",
      "ì²­í¬ 20: 498ì\n",
      "ì²­í¬ 21: 285ì\n",
      "ì²­í¬ 22: 494ì\n",
      "ì²­í¬ 23: 494ì\n",
      "ì²­í¬ 24: 497ì\n",
      "ì²­í¬ 25: 492ì\n",
      "ì²­í¬ 26: 498ì\n",
      "ì²­í¬ 27: 416ì\n",
      "ì²­í¬ 28: 498ì\n",
      "ì²­í¬ 29: 88ì\n",
      "ì²­í¬ 30: 494ì\n",
      "ì²­í¬ 31: 496ì\n",
      "ì²­í¬ 32: 490ì\n",
      "ì²­í¬ 33: 496ì\n",
      "ì²­í¬ 34: 153ì\n",
      "ì²­í¬ 35: 498ì\n",
      "ì²­í¬ 36: 496ì\n",
      "ì²­í¬ 37: 499ì\n",
      "ì²­í¬ 38: 487ì\n",
      "ì²­í¬ 39: 494ì\n",
      "ì²­í¬ 40: 418ì\n",
      "ì²­í¬ 41: 499ì\n",
      "ì²­í¬ 42: 498ì\n",
      "ì²­í¬ 43: 497ì\n",
      "ì²­í¬ 44: 305ì\n",
      "ì²­í¬ 45: 496ì\n",
      "ì²­í¬ 46: 495ì\n",
      "ì²­í¬ 47: 497ì\n",
      "ì²­í¬ 48: 493ì\n",
      "ì²­í¬ 49: 499ì\n",
      "ì²­í¬ 50: 205ì\n",
      "ì²­í¬ 51: 499ì\n",
      "ì²­í¬ 52: 496ì\n",
      "ì²­í¬ 53: 497ì\n",
      "ì²­í¬ 54: 493ì\n",
      "ì²­í¬ 55: 496ì\n",
      "ì²­í¬ 56: 496ì\n",
      "ì²­í¬ 57: 498ì\n",
      "ì²­í¬ 58: 68ì\n",
      "ì²­í¬ 59: 496ì\n",
      "ì²­í¬ 60: 496ì\n",
      "ì²­í¬ 61: 499ì\n",
      "ì²­í¬ 62: 80ì\n",
      "ì²­í¬ 63: 499ì\n",
      "ì²­í¬ 64: 499ì\n",
      "ì²­í¬ 65: 499ì\n",
      "ì²­í¬ 66: 498ì\n",
      "ì²­í¬ 67: 492ì\n",
      "ì²­í¬ 68: 492ì\n",
      "ì²­í¬ 69: 491ì\n",
      "ì²­í¬ 70: 493ì\n",
      "ì²­í¬ 71: 492ì\n",
      "ì²­í¬ 72: 495ì\n",
      "ì²­í¬ 73: 498ì\n",
      "ì²­í¬ 74: 497ì\n",
      "ì²­í¬ 75: 321ì\n",
      "ì²­í¬ 76: 496ì\n",
      "ì²­í¬ 77: 496ì\n",
      "ì²­í¬ 78: 499ì\n",
      "ì²­í¬ 79: 498ì\n",
      "ì²­í¬ 80: 499ì\n",
      "ì²­í¬ 81: 494ì\n",
      "ì²­í¬ 82: 496ì\n",
      "ì²­í¬ 83: 498ì\n",
      "ì²­í¬ 84: 494ì\n",
      "ì²­í¬ 85: 494ì\n",
      "ì²­í¬ 86: 273ì\n",
      "ì²­í¬ 87: 493ì\n",
      "ì²­í¬ 88: 494ì\n",
      "ì²­í¬ 89: 497ì\n",
      "ì²­í¬ 90: 496ì\n",
      "ì²­í¬ 91: 497ì\n",
      "ì²­í¬ 92: 494ì\n",
      "ì²­í¬ 93: 496ì\n",
      "ì²­í¬ 94: 497ì\n",
      "ì²­í¬ 95: 499ì\n",
      "ì²­í¬ 96: 496ì\n",
      "ì²­í¬ 97: 494ì\n",
      "ì²­í¬ 98: 497ì\n",
      "ì²­í¬ 99: 496ì\n",
      "ì²­í¬ 100: 470ì\n",
      "ì²­í¬ 101: 490ì\n",
      "ì²­í¬ 102: 496ì\n",
      "ì²­í¬ 103: 97ì\n",
      "ì²­í¬ 104: 498ì\n",
      "ì²­í¬ 105: 492ì\n",
      "ì²­í¬ 106: 498ì\n",
      "ì²­í¬ 107: 488ì\n",
      "ì²­í¬ 108: 495ì\n",
      "ì²­í¬ 109: 495ì\n",
      "ì²­í¬ 110: 494ì\n",
      "ì²­í¬ 111: 485ì\n",
      "ì²­í¬ 112: 496ì\n",
      "ì²­í¬ 113: 499ì\n",
      "ì²­í¬ 114: 496ì\n",
      "ì²­í¬ 115: 491ì\n",
      "ì²­í¬ 116: 217ì\n",
      "ì²­í¬ 117: 192ì\n",
      "ì²­í¬ 118: 499ì\n",
      "ì²­í¬ 119: 494ì\n",
      "ì²­í¬ 120: 495ì\n",
      "ì²­í¬ 121: 495ì\n",
      "ì²­í¬ 122: 497ì\n",
      "ì²­í¬ 123: 140ì\n",
      "ì²­í¬ 124: 499ì\n",
      "ì²­í¬ 125: 497ì\n",
      "ì²­í¬ 126: 493ì\n",
      "ì²­í¬ 127: 499ì\n",
      "ì²­í¬ 128: 281ì\n",
      "ì²­í¬ 129: 497ì\n",
      "ì²­í¬ 130: 498ì\n",
      "ì²­í¬ 131: 480ì\n",
      "ì²­í¬ 132: 495ì\n",
      "ì²­í¬ 133: 278ì\n",
      "ì²­í¬ 134: 492ì\n",
      "ì²­í¬ 135: 499ì\n",
      "ì²­í¬ 136: 462ì\n",
      "ì²­í¬ 137: 498ì\n",
      "ì²­í¬ 138: 497ì\n",
      "ì²­í¬ 139: 499ì\n",
      "ì²­í¬ 140: 497ì\n",
      "ì²­í¬ 141: 491ì\n",
      "ì²­í¬ 142: 498ì\n",
      "ì²­í¬ 143: 474ì\n",
      "ì²­í¬ 144: 498ì\n",
      "ì²­í¬ 145: 497ì\n",
      "ì²­í¬ 146: 173ì\n",
      "ì²­í¬ 147: 383ì\n",
      "ì²­í¬ 148: 251ì\n",
      "ì²­í¬ 149: 465ì\n",
      "ì²­í¬ 150: 499ì\n",
      "ì²­í¬ 151: 446ì\n",
      "ì²­í¬ 152: 495ì\n",
      "ì²­í¬ 153: 62ì\n",
      "ì²­í¬ 154: 489ì\n",
      "ì²­í¬ 155: 497ì\n",
      "ì²­í¬ 156: 495ì\n",
      "ì²­í¬ 157: 496ì\n",
      "ì²­í¬ 158: 68ì\n",
      "ì²­í¬ 159: 230ì\n",
      "ì²­í¬ 160: 499ì\n",
      "ì²­í¬ 161: 408ì\n",
      "ì²­í¬ 162: 496ì\n",
      "ì²­í¬ 163: 495ì\n",
      "ì²­í¬ 164: 498ì\n",
      "ì²­í¬ 165: 153ì\n",
      "ì²­í¬ 166: 294ì\n",
      "ì²­í¬ 167: 493ì\n",
      "ì²­í¬ 168: 282ì\n",
      "ì²­í¬ 169: 296ì\n",
      "ì²­í¬ 170: 498ì\n",
      "ì²­í¬ 171: 232ì\n",
      "ì²­í¬ 172: 500ì\n",
      "ì²­í¬ 173: 488ì\n",
      "ì²­í¬ 174: 494ì\n",
      "ì²­í¬ 175: 494ì\n",
      "ì²­í¬ 176: 499ì\n",
      "ì²­í¬ 177: 491ì\n",
      "ì²­í¬ 178: 111ì\n",
      "ì²­í¬ 179: 497ì\n",
      "ì²­í¬ 180: 86ì\n",
      "ì²­í¬ 181: 489ì\n",
      "ì²­í¬ 182: 470ì\n",
      "ì²­í¬ 183: 443ì\n",
      "ì²­í¬ 184: 495ì\n",
      "ì²­í¬ 185: 492ì\n",
      "ì²­í¬ 186: 101ì\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "docs = splitter.split_documents(documents)\n",
    "\n",
    "print(f\"ì´ chunk ìˆ˜: {len(docs)}\")\n",
    "# ê° ì²­í¬ì˜ ê¸€ì ìˆ˜ í™•ì¸\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    print(f\"ì²­í¬ {i}: {len(doc.page_content)}ì\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0c30ec8-8dad-4a7b-a862-0a241fb4b8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': './PromptEngineering.pdf', 'page': 10, 'heading': 'LLM output configuration', 'subheading': 'Sampling controls', 'sub_subheading': 'Temperature'}\n"
     ]
    }
   ],
   "source": [
    "# heading, subheading, sub_subheading í™•ì¸\n",
    "print(docs[17].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "728223ac-2c33-47c6-a618-fe0086607a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content='temperature with high certainty. A higher Gemini temperature setting is like a high softmax temperature, making a wider range of temperatures around the selected setting more acceptable. This increased uncertainty accommodates scenarios where a rigid, precise temperature may not be essential like for example when experimenting with creative outputs.' metadata={'title': './PromptEngineering.pdf', 'page': 10, 'heading': 'LLM output configuration', 'subheading': 'Sampling controls', 'sub_subheading': 'Temperature'}\n"
     ]
    }
   ],
   "source": [
    "print(docs[17])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2772a8-53a6-4095-b016-2cb75e96ad66",
   "metadata": {},
   "source": [
    "## 5. Qdrant ë²¡í„°DB ìƒì„± ë˜ëŠ” ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ec45151c-557f-4e48-b698-f4c4744e7197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ê¸°ì¡´ ì»¬ë ‰ì…˜ 'qdrant_0526'ì— ì—°ê²°í•©ë‹ˆë‹¤.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\wiseai\\anaconda3\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:119: LangChainDeprecationWarning: The class `Qdrant` was deprecated in LangChain 0.0.37 and will be removed in 0.3.0. An updated version of the class exists in the langchain-qdrant package and should be used instead. To use it run `pip install -U langchain-qdrant` and import as `from langchain_qdrant import Qdrant`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import CollectionStatus\n",
    "\n",
    "# ì„ë² ë”© ëª¨ë¸ ì„¤ì •\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# ë¡œì»¬ ë„ì»¤ë¡œ ì‹¤í–‰ ì¤‘ì¸ Qdrantì— ì—°ê²°\n",
    "client = QdrantClient(url=\"http://localhost:6333\")\n",
    "\n",
    "# í˜„ì¬ ì¡´ì¬í•˜ëŠ” ì»¬ë ‰ì…˜ ëª©ë¡ í™•ì¸\n",
    "existing_collections = [col.name for col in client.get_collections().collections]\n",
    "#print(existing_collections)\n",
    "\n",
    "# ì‚¬ìš©í•  ì»¬ë ‰ì…˜ ì´ë¦„\n",
    "collection_name = \"qdrant_0526\"\n",
    "\n",
    "if collection_name in existing_collections:\n",
    "    print(f\"ê¸°ì¡´ ì»¬ë ‰ì…˜ '{collection_name}'ì— ì—°ê²°í•©ë‹ˆë‹¤.\")\n",
    "    qdrant = Qdrant(\n",
    "        client=client,\n",
    "        collection_name=collection_name,\n",
    "        embeddings=embeddings,\n",
    "    )\n",
    "else:\n",
    "    print(f\"ì»¬ë ‰ì…˜ '{collection_name}'ì„ ìƒì„±í•˜ê³  ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•©ë‹ˆë‹¤.\")\n",
    "    qdrant = Qdrant.from_documents(\n",
    "        documents=docs,\n",
    "        embedding=embeddings,\n",
    "        url=\"http://localhost:6333\",\n",
    "        prefer_grpc=True,  # gRPC ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ True ì¶”ì²œ\n",
    "        collection_name=collection_name,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510728e7-60aa-44c4-9db4-3181c9230f41",
   "metadata": {},
   "source": [
    "## 6. ì¿¼ë¦¬ì— ëŒ€í•œ ë‹µë³€ ìƒì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c425bc60-6e7a-445f-b7e2-54f8243890b5",
   "metadata": {},
   "source": [
    "#### Qdrantì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd39864f-6ee3-4a47-a2a7-21570a7b132b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------- ë¬¸ì„œ 1 -------\n",
      "ìœ ì‚¬ë„ ì ìˆ˜: 0.4728\n",
      "ëŒ€ëª©ì°¨: Prompting techniques\n",
      "ì¤‘ëª©ì°¨: Tree of Thoughts (ToT)\n",
      "ì†Œëª©ì°¨: None\n",
      "í˜ì´ì§€: 37\n",
      "ë‚´ìš©: Now that we are familiar with chain of thought and self-consistency prompting, letâ€™s review Tree of Thoughts (ToT).12 It generalizes the concept of CoT prompting because it allows LLMs to explore multiple different reasoning paths simultaneously, rather than just following a single linear chain of thought. This is depicted in Figure 1. Figure 1. A visualization of chain of thought prompting on the left versus. Tree of Thoughts prompting on the right This approach makes ToT particularly\n",
      "\n",
      "------- ë¬¸ì„œ 2 -------\n",
      "ìœ ì‚¬ë„ ì ìˆ˜: 0.4451\n",
      "ëŒ€ëª©ì°¨: Prompting techniques\n",
      "ì¤‘ëª©ì°¨: Chain of Thought (CoT)\n",
      "ì†Œëª©ì°¨: None\n",
      "í˜ì´ì§€: 32\n",
      "ë‚´ìš©: Chain of Thought (CoT) 9 prompting is a technique for improving the reasoning capabilities of LLMs by generating intermediate reasoning steps. This helps the LLM generate more accurate answers. You can combine it with few-shot prompting to get better results on more complex tasks that require reasoning before responding as itâ€™s a challenge with a zero-shot chain of thought. CoT has a lot of advantages. First of all, itâ€™s low-effort while being very effective and works well with off-the-shelf\n",
      "\n",
      "------- ë¬¸ì„œ 3 -------\n",
      "ìœ ì‚¬ë„ ì ìˆ˜: 0.4345\n",
      "ëŒ€ëª©ì°¨: Prompting techniques\n",
      "ì¤‘ëª©ì°¨: Tree of Thoughts (ToT)\n",
      "ì†Œëª©ì°¨: None\n",
      "í˜ì´ì§€: 37\n",
      "ë‚´ìš©: on the right This approach makes ToT particularly well-suited for complex tasks that require exploration. It works by maintaining a tree of thoughts, where each thought represents a coherent language sequence that serves as an intermediate step toward solving a problem. The model can then explore different reasoning paths by branching out from different nodes in the tree. Thereâ€™s a great notebook, which goes into a bit more detail showing The Tree of Thought (ToT) which is based on the paper\n",
      "\n",
      "------- ë¬¸ì„œ 4 -------\n",
      "ìœ ì‚¬ë„ ì ìˆ˜: 0.4219\n",
      "ëŒ€ëª©ì°¨: Best Practices\n",
      "ì¤‘ëª©ì°¨: CoT Best practices\n",
      "ì†Œëª©ì°¨: None\n",
      "í˜ì´ì§€: 61\n",
      "ë‚´ìš©: For CoT prompting, putting the answer after the reasoning is required because the generation of the reasoning changes the tokens that the model gets when it predicts the final answer. With CoT and self-consistency you need to be able to extract the final answer from your prompt, separated from the reasoning. For CoT prompting, set the temperature to 0. Chain of thought prompting is based on greedy decoding, predicting the next word in a sequence based on the highest probability assigned by the\n",
      "\n",
      "------- ë¬¸ì„œ 5 -------\n",
      "ìœ ì‚¬ë„ ì ìˆ˜: 0.4108\n",
      "ëŒ€ëª©ì°¨: Prompting techniques\n",
      "ì¤‘ëª©ì°¨: Chain of Thought (CoT)\n",
      "ì†Œëª©ì°¨: None\n",
      "í˜ì´ì§€: 32\n",
      "ë‚´ìš©: chain of thought. Please refer to the notebook10 hosted in the GoogleCloudPlatform Github repository which will go into further detail on CoT prompting: In the best practices section of this chapter, we will learn some best practices specific to Chain of thought prompting.\n"
     ]
    }
   ],
   "source": [
    "query = \"CoTì™€ ToTë¥¼ ë¹„êµí•´ì„œ ì„¤ëª…í•´ì¤˜\"\n",
    "retrieved_docs = qdrant.similarity_search_with_score(query, k=5)\n",
    "\n",
    "for i, (doc, score) in enumerate(retrieved_docs, 1):\n",
    "    print(f\"\\n------- ë¬¸ì„œ {i} -------\")\n",
    "    print(f\"ìœ ì‚¬ë„ ì ìˆ˜: {score:.4f}\")\n",
    "    print(f\"ëŒ€ëª©ì°¨: {doc.metadata.get('heading')}\")\n",
    "    print(f\"ì¤‘ëª©ì°¨: {doc.metadata.get('subheading')}\")\n",
    "    print(f\"ì†Œëª©ì°¨: {doc.metadata.get('sub_subheading')}\")\n",
    "    print(f\"í˜ì´ì§€: {doc.metadata.get('page')}\")\n",
    "    print(f\"ë‚´ìš©: {doc.page_content[:1000]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "263ab69d-2935-4027-991c-139699817a24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5ê°œ ë¬¸ì„œ ê²€ìƒ‰ì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ìœ ì‚¬ë„ ì ìˆ˜ 0.4 ì´ìƒìœ¼ë¡œ í•„í„°ë§ ë° ìœ ì‚¬ë„ ì ìˆ˜ ì œê±°\n",
    "filtered_docs = [doc for doc, score in retrieved_docs if score >= 0.4]\n",
    "if not filtered_docs:\n",
    "    print(\"ê²€ìƒ‰ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.\")\n",
    "else:\n",
    "    print(f\"{len(filtered_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ì— ì„±ê³µí–ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03018c4-1826-477a-8dff-90194f7ba0cc",
   "metadata": {},
   "source": [
    "#### ë‹µë³€ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f4bd382-2283-42ce-ab1b-4000575ae152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â“ ì§ˆë¬¸: CoTì™€ ToTë¥¼ ë¹„êµí•´ì„œ ì„¤ëª…í•´ì¤˜\n",
      "\n",
      "====================================================================================================\n",
      "\n",
      "ğŸ’¡ ë‹µë³€: ë„¤, Chain of Thought(CoT)ì™€ Tree of Thoughts(ToT)ë¥¼ ë¹„êµí•´ì„œ ì„¤ëª…ë“œë¦¬ê² ìŠµë‹ˆë‹¤.\n",
      "\n",
      "### 1. Chain of Thought (CoT)\n",
      "- **ê°œë…**: CoTëŠ” ëŒ€í˜• ì–¸ì–´ ëª¨ë¸(LLM)ì´ ë¬¸ì œë¥¼ í•´ê²°í•  ë•Œ, ì¤‘ê°„ ì¶”ë¡  ê³¼ì •ì„ ë‹¨ê³„ë³„ë¡œ ìƒì„±í•˜ë„ë¡ ìœ ë„í•˜ëŠ” í”„ë¡¬í”„íŠ¸ ê¸°ë²•ì…ë‹ˆë‹¤.\n",
      "- **ë°©ì‹**: í•œ ë²ˆì— í•œ ê²½ë¡œ(ì„ í˜•ì , ì¼ì§ì„ )ì˜ ì¶”ë¡ ë§Œì„ ë”°ë¼ê°€ë©°, ê° ë‹¨ê³„ë³„ë¡œ ë…¼ë¦¬ì  ì´ìœ ë¥¼ ì°¨ë¡€ë¡œ ë‚˜ì—´í•©ë‹ˆë‹¤.\n",
      "- **ì¥ì **: êµ¬í˜„ì´ ê°„ë‹¨í•˜ê³ , ê¸°ì¡´ LLMì— ë°”ë¡œ ì ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ë³µì¡í•œ ë¬¸ì œì— ëŒ€í•´ ë” ì •í™•í•œ ë‹µë³€ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- **ì ìš© ì˜ˆì‹œ**: ìˆ˜í•™ ë¬¸ì œ í’€ì´, ë…¼ë¦¬ì  ì¶”ë¡  ë¬¸ì œ ë“±ì—ì„œ \"ìƒê°ì˜ íë¦„\"ì„ ë‹¨ê³„ë³„ë¡œ ë³´ì—¬ì¤ë‹ˆë‹¤.\n",
      "\n",
      "### 2. Tree of Thoughts (ToT)\n",
      "- **ê°œë…**: ToTëŠ” CoTë¥¼ ì¼ë°˜í™”í•œ ê¸°ë²•ìœ¼ë¡œ, LLMì´ ì—¬ëŸ¬ ê°œì˜ ì¶”ë¡  ê²½ë¡œ(ê°€ì§€)ë¥¼ ë™ì‹œì— íƒìƒ‰í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤.\n",
      "- **ë°©ì‹**: ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê³¼ì •ì—ì„œ í•˜ë‚˜ì˜ ì„ í˜• ê²½ë¡œê°€ ì•„ë‹ˆë¼, ì—¬ëŸ¬ ê°€ì§€ ê°€ëŠ¥í•œ ì¤‘ê°„ ë‹¨ê³„(ë…¸ë“œ)ë¥¼ íŠ¸ë¦¬ êµ¬ì¡°ë¡œ í™•ì¥í•˜ë©° íƒìƒ‰í•©ë‹ˆë‹¤. ê° ë…¸ë“œëŠ” í•˜ë‚˜ì˜ \"ìƒê°\"ì„ ë‚˜íƒ€ë‚´ê³ , ì—¬ëŸ¬ ë…¸ë“œì—ì„œ ìƒˆë¡œìš´ ê°€ì§€ë¡œ ë¶„ê¸°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "- **ì¥ì **: ë³µì¡í•˜ê³  íƒìƒ‰ì´ í•„ìš”í•œ ë¬¸ì œ(ì˜ˆ: ì°½ì˜ì  ë¬¸ì œ í•´ê²°, ê³„íš ìˆ˜ë¦½ ë“±)ì— ë” ì í•©í•©ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê²½ë¡œë¥¼ ë™ì‹œì— ê³ ë ¤í•˜ë¯€ë¡œ ë” ë‚˜ì€ í•´ë‹µì„ ì°¾ì„ í™•ë¥ ì´ ë†’ì•„ì§‘ë‹ˆë‹¤.\n",
      "- **ì ìš© ì˜ˆì‹œ**: ì—¬ëŸ¬ ê°€ì§€ í•´ê²°ì±…ì„ ë¹„êµí•˜ê±°ë‚˜, ë‹¤ì–‘í•œ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë™ì‹œì— ê³ ë ¤í•´ì•¼ í•˜ëŠ” ë¬¸ì œì— íš¨ê³¼ì ì…ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "### ìš”ì•½ ë¹„êµ\n",
      "\n",
      "| êµ¬ë¶„         | Chain of Thought (CoT)         | Tree of Thoughts (ToT)           |\n",
      "|--------------|-------------------------------|-----------------------------------|\n",
      "| ì¶”ë¡  ê²½ë¡œ    | í•œ ê°œ(ì„ í˜•)                   | ì—¬ëŸ¬ ê°œ(íŠ¸ë¦¬ êµ¬ì¡°)                |\n",
      "| íƒìƒ‰ ë°©ì‹    | ë‹¨ê³„ë³„ë¡œ í•œ ë°©í–¥ë§Œ ì§„í–‰        | ì—¬ëŸ¬ ë°©í–¥ìœ¼ë¡œ ë¶„ê¸°í•˜ë©° íƒìƒ‰        |\n",
      "| ì¥ì          | ê°„ë‹¨, íš¨ê³¼ì , ì ìš© ì‰¬ì›€        | ë³µì¡í•œ ë¬¸ì œì— ê°•í•¨, ë‹¤ì–‘í•œ í•´ë²• íƒìƒ‰ |\n",
      "| í™œìš© ì˜ˆì‹œ    | ìˆ˜í•™ í’€ì´, ë…¼ë¦¬ ë¬¸ì œ           | ì°½ì˜ì  ë¬¸ì œ, ê³„íš, ì‹œë‚˜ë¦¬ì˜¤ ë¶„ì„   |\n",
      "\n",
      "---\n",
      "\n",
      "**ì •ë¦¬:**  \n",
      "CoTëŠ” í•œ ê°€ì§€ ìƒê°ì˜ íë¦„ì„ ë”°ë¼ê°€ë©° ë¬¸ì œë¥¼ í‘¸ëŠ” ë°©ì‹ì´ê³ , ToTëŠ” ì—¬ëŸ¬ ê°€ì§€ ìƒê°ì˜ íë¦„ì„ ë™ì‹œì— íƒìƒ‰í•˜ì—¬ ë” ë³µì¡í•œ ë¬¸ì œì— ì í•©í•œ ë°©ì‹ì…ë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import HumanMessage\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# LLM ì´ˆê¸°í™”\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4.1\")\n",
    "\n",
    "# QA ì²´ì¸ ë¶ˆëŸ¬ì˜¤ê¸° (chain_type: \"stuff\", \"map_reduce\", \"refine\" ì¤‘ ì„ íƒ)\n",
    "qa_chain = load_qa_chain(llm, chain_type=\"stuff\")\n",
    "\n",
    "# ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±\n",
    "answer = qa_chain.invoke({\"input_documents\": filtered_docs, \"question\": query})\n",
    "\n",
    "print(\"â“ ì§ˆë¬¸:\", answer.get(\"question\", \"\"))\n",
    "print(\"\\n\" + \"=\"*100 + \"\\n\")\n",
    "print(\"ğŸ’¡ ë‹µë³€:\", answer.get(\"output_text\", \"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
